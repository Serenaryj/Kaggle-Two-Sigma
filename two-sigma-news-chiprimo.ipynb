{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Changes compared to version V10 - 0.70951\n# 1. this version fixes the bug in creating lag features in prediction part\n# 2. adds lag terms of a few more market features\n# 3. restructures the code\n# 4. 2-lags is the best\nfrom kaggle.competitions import twosigmanews\nimport numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.isotonic import IsotonicRegression\n\nfrom itertools import product\n\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2ae327bd46bb82ae019d1d6ca87be19b66df858"},"cell_type":"code","source":"import logging\nlogging.getLogger().setLevel(logging.DEBUG)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def generate_split_dates(start_year, end_year):\n    \"\"\" Generate a list of start & end dates for splitting the data (e.g. [('2016-01-01','2016-12-31')])\n    \"\"\"\n    split_dates = []\n    for year in range(start_year, end_year+1):\n        split_dates.append((str(year)+'-01-01', str(year)+'-12-31'))\n    return split_dates","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96add32ebf75cd5bd8b6a3ce890b94cc9c0d76ca","trusted":true},"cell_type":"code","source":"def split_data_by_date(data, split_dates):\n    \"\"\" Split the data by the input list of dates\n    data: the input DataFrame\n    split_dates: a list of start & end dates for splitting the data (e.g. [('2016-01-01','2016-12-31')])\n    \"\"\"\n    data_split = {}\n    for start_date, end_date in split_dates:\n        condition1 = data['time'] >= pd.to_datetime(start_date).date()\n        condition2 = data['time'] <= pd.to_datetime(end_date).date()\n        data_split[start_date] = data[condition1 & condition2]\n    return data_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a180729344021439c77d0534baa5960d9ca78144","trusted":true},"cell_type":"code","source":"def find_asset_name_map(market_train, news_train):\n    \"\"\" Find assetName correspondences between market_train and news_train\n    \"\"\"\n    # Get assetCode and assetName for both market_train and news_train data\n    code_name_market = market_train[['assetCode', 'assetName']].drop_duplicates(subset=['assetCode'])\n    code_name_news = news_train[['assetCodes', 'assetName']].drop_duplicates(subset=['assetCodes', 'assetName'])\n    # Split \"assetCodes\"\n    code_name_news['assetCodes'] = code_name_news['assetCodes'].str.strip('{}').str.split(',')\n    # For each assetCode in the list of \"assetCodes\", prepare a new row for each assetcode\n    assetCode = code_name_news.apply(lambda x: pd.Series(x['assetCodes']), axis=1).stack().reset_index(level=1, drop=True)\n    assetCode.name = 'assetCode'\n    code_name_news.drop('assetCodes', axis=1, inplace=True)\n    code_name_news_joined = code_name_news.join(assetCode).reset_index(drop=True)\n    code_name_news_joined['assetCode'] = code_name_news_joined['assetCode'].apply(lambda x: x.replace(\"'\",\"\"))\n    code_name_news_joined['assetCode'] = code_name_news_joined['assetCode'].apply(lambda x: x.replace(\" \",\"\"))\n    # Rename assetName to assetName_news\n    code_name_news_joined.rename(columns={'assetName': 'assetName_news'}, inplace=True)\n    # Merge two dataframes together\n    code_name_merged = pd.merge(left=code_name_market, right=code_name_news_joined, how='left', on=['assetCode'])\n    # Filter out Unknown and NAN assetName\n    code_name_merged.dropna(inplace=True)\n    condition = code_name_merged['assetName'] == 'Unknown'\n    code_name_filtered = code_name_merged[~condition]\n    # Build up a dictionary to establish assetName mapping from market_train to news_train\n    assetName_map = {}\n    # Note: iterrows can be very inefficient!\n    for index, row in code_name_filtered.iterrows():\n        if row['assetName'] != row['assetName_news']:\n            assetName_map[row['assetName_news']] = row['assetName']\n    return assetName_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76b8c9da47450334e72f2f811f42f6f24308d9ee"},"cell_type":"code","source":"def normalize_news_sentiment(news_train):\n    \"\"\" Normalize news sentiment by subtracting the mean sentiment of each day\n    \"\"\"\n    for date in news_train['time'].unique().tolist():\n        # Normalize all news sentiment for the given date\n        mean_sentiment_positive = np.mean(news_train.loc[news_train.time == date, 'sentimentPositive'])\n        news_train.loc[news_train.time == date, 'sentimentPositive'] = news_train.loc[news_train.time == date, 'sentimentPositive'] - mean_sentiment_positive\n        mean_sentiment_negative = np.mean(news_train.loc[news_train.time == date, 'sentimentNegative'])\n        news_train.loc[news_train.time == date, 'sentimentNegative'] = news_train.loc[news_train.time == date, 'sentimentNegative'] - mean_sentiment_negative\n        mean_sentiment_neutral = np.mean(news_train.loc[news_train.time == date, 'sentimentNeutral'])\n        news_train.loc[news_train.time == date, 'sentimentNeutral'] = news_train.loc[news_train.time == date, 'sentimentNeutral'] - mean_sentiment_neutral","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f9c07e473782a4a609846e64e2a852f20c022e3"},"cell_type":"code","source":"def merge_by_asset_name(market_train, news_train, assetName_map):\n    # Modify assetName in news data according to assetName_map\n    news_train['assetName'] = news_train['assetName'].apply(lambda x: assetName_map[x] if x in assetName_map.keys() else x)\n    news_train.drop(['assetCodes'], axis=1, inplace=True)\n    # Group news_train by \"time\" and \"assetCode\" and then compute mean on each group\n    news_train_grouped = news_train.groupby(['time','assetName'], sort=False).aggregate(np.mean).reset_index()\n    # Normalize news sentiment\n    normalize_news_sentiment(news_train_grouped)\n    # Merge two DataFrames\n    return pd.merge(left=market_train, right= news_train_grouped, how='left', on=['time', 'assetName'], copy=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8b2f6aae077488813fb1d58f26d91096e4bcfbe","trusted":true},"cell_type":"code","source":"def merge_data(market_train, news_train, split_by_year=True):\n    \"\"\" Return the combined data by merging market_train and news_train on \"time\" and \"assetCode\"\n    \"\"\"\n    \"\"\" Pre-process of market_train \"\"\"\n    # Convert \"time\" to datetime format (Note: Currently, we only keep the time to date)\n    market_train['time'] = pd.to_datetime(market_train['time']).apply(lambda x: x.date())\n    #logging.debug('Convert time to datetime format is done for market data!')\n    \n    \"\"\" Pre-process of news_train \"\"\"\n    # Convert \"time\" to datetime format (Note: Currently, we only keep the time to date)\n    news_train['time'] = pd.to_datetime(news_train['time']).apply(lambda x: x.date())\n    #logging.debug('Convert time to datetime format is done for news data!')\n\n    # feature engineering before dropping columns\n    news_train['position'] = news_train['firstMentionSentence'] / news_train['sentenceCount']\n    news_train['coverage'] = news_train['sentimentWordCount'] / news_train['wordCount']\n    \n    # Get rid of some columns in news data (the list of dropped columns can be modified)\n    drop_list = ['sourceTimestamp','firstCreated','sourceId','headline',\n                 'takeSequence','provider','firstMentionSentence',\n                 'sentenceCount','headlineTag','marketCommentary',\n                 'subjects','audiences','wordCount','sentimentWordCount']\n    \n    news_train.drop(drop_list, axis=1, inplace=True)\n    #logging.debug('Drop columns is done for news data!')\n    \n    # Find assetName map from market_train to news_train\n    assetName_map = find_asset_name_map(market_train, news_train)\n    #logging.debug('Find the assetName correspondences between market and news data!')\n    \n    # Adjust 'time' for news_train\n    # First, get all the unique dates from news data and market data\n    time_market = pd.DataFrame(market_train['time'].unique(), columns={'time'})\n    time_news = pd.DataFrame(news_train['time'].unique(), columns={'time'})\n    # Keep a copy of market date before merging\n    time_market['time_market'] = time_market['time']\n    # Merge the two dataframes,the merged dataframe should have the same length with time_news\n    # Also fill the next trading date\n    time_adjusted= pd.merge(left=time_market, right= time_news, how='right', on=['time'], sort=True).fillna(method='bfill')\n    # Merge adjusted time to news data\n    news_train_adjusted = pd.merge(left=news_train, right=time_adjusted, how='left', on=['time'], copy=False)\n    del news_train\n    # Modify 'time_market' as the new 'time' column\n    news_train_adjusted.drop(['time'], axis=1, inplace=True)\n    news_train_adjusted.rename(columns={'time_market': 'time'}, inplace=True)\n    #logging.debug('Adjust date is done for news data!')\n    \n    # Split market_train and news_train by year\n    if (split_by_year):\n        market_train_years = pd.to_datetime(market_train['time']).dt.year.unique()\n        news_train_years = pd.to_datetime(news_train_adjusted['time']).dt.year.unique()\n        start_year = min(np.amin(market_train_years), np.amin(news_train_years))\n        end_year = max(np.amax(market_train_years), np.amax(news_train_years))\n        #logging.debug('Split data from year-%d to year-%d' % (start_year, end_year))\n        split_dates = generate_split_dates(start_year, end_year)\n        market_train_split = split_data_by_date(market_train, split_dates)\n        del market_train\n        #logging.debug('Split market data is done!')\n        news_train_split = split_data_by_date(news_train_adjusted, split_dates)\n        del news_train_adjusted\n        #logging.debug('Split news data is done!')\n        # Iterate over split market and news data\n        if len(market_train_split.items()) != len(news_train_split.items()):\n            raise ValueError('The split train and news data must have the same length!')  \n        merged_data = pd.DataFrame([])\n        for start_date, end_date in split_dates:\n            logging.debug('Merge data from %s to %s ...' % (start_date, end_date))\n            market_train_to_process = market_train_split[start_date]\n            news_train_to_process = news_train_split[start_date]\n            # Merge two DataFrames\n            merged_data = pd.concat([merged_data, \n                                     merge_by_asset_name(market_train_to_process, news_train_to_process, assetName_map)],\n                                     ignore_index=True)\n            #merged_data.info()\n            del market_train_to_process\n            del news_train_to_process\n        del market_train_split\n        del news_train_split\n    else: \n        # If we do not split data by year, we should directly merge market and news train data.\n        merged_data = merge_by_asset_name(market_train, news_train_adjusted, assetName_map)\n    # Concatenate and return all DataFrames\n    return merged_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"580eed1cd0ced8eba0b544534a3e3b9ddbff7fe3","trusted":true},"cell_type":"code","source":"# Handling missing values\ndef handle_missing_values(merged_data):\n    # Market data: replacing NaN by implied market-adjusted returns\n    Raw_cols = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n    Mktres_cols = ['returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    Mkt_cols = ['returnClosePrevMkt1', 'returnOpenPrevMkt1', 'returnClosePrevMkt10', 'returnOpenPrevMkt10']\n    for i in range(len(Mkt_cols)):\n        merged_data[Mkt_cols[i]] = merged_data[Raw_cols[i]] - merged_data[Mktres_cols[i]]\n        merged_data[Mkt_cols[i]] = merged_data.groupby('time')[Mkt_cols[i]].transform(lambda x: x.mean())\n        merged_data[Mktres_cols[i]] = merged_data[Mktres_cols[i]].fillna(merged_data[Raw_cols[i]] - merged_data[Mkt_cols[i]])\n    \n    # News data: adding a 'nonews' feature & then, replacing NaN by a fixed constant\n    merged_data['noNews'] = merged_data.sentimentPositive.isnull().astype(int)\n    merged_data.fillna(-999, inplace=True)\n    assert merged_data.isnull().sum().sum() == 0\n    return merged_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e154b4f9e2354635f42b9169963915267c1c4a9","trusted":true},"cell_type":"code","source":"# Prepare merged data for training & prediction\ndef prepare_data(merged_data, goal, lag_features_window_10, lag_features_window_1=[], n_lag=2):\n    ## Handling missing values\n    merged_data = handle_missing_values(merged_data)\n    \n    ## Feature engineering\n    # market features:\n    merged_data['daytrend'] = merged_data['close'] / merged_data['open']\n    #merged_data['average'] = (merged_data['close'] + merged_data['open']) / 2\n    #merged_data['moving_average_10_days'] = merged_data.groupby('assetCode')['open'].transform(lambda x: pd.Series.ewm(x, span=10).mean())\n    #merged_data['moving_average_20_days'] = merged_data.groupby('assetCode')['open'].transform(lambda x: pd.Series.ewm(x, span=20).mean())\n    #merged_data['moving_average_50_days'] = merged_data.groupby('assetCode')['open'].transform(lambda x: pd.Series.ewm(x, span=50).mean())\n    #merged_data['moving_average_100_days'] = merged_data.groupby('assetCode')['open'].transform(lambda x: pd.Series.ewm(x, span=100).mean())\n    #merged_data['moving_average_200_days'] = merged_data.groupby('assetCode')['open'].transform(lambda x: pd.Series.ewm(x, span=200).mean())\n    \n    if goal == 'pred':\n        for i in range(n_lag):\n            for feature in lag_features_window_10 + lag_features_window_1:\n                merged_data['%s_lag_%s' % (feature, i+1)] = np.nan # will input values later in prediction part\n    else: # elif goal == 'train'\n        for i in range(n_lag):\n            for feature in lag_features_window_10:\n                merged_data['%s_lag_%s' % (feature, i+1)] = merged_data.groupby('assetCode')['%s' % feature].transform(lambda x: x.shift(10 * (i+1)).fillna(0))\n            for feature in lag_features_window_1:\n                merged_data['%s_lag_%s' % (feature, i+1)] = merged_data.groupby('assetCode')['%s' % feature].transform(lambda x: x.shift(1 * (i+1)).fillna(0))\n                \n            #merged_data['%s_lag_2' % feature] = merged_data.groupby('assetCode')['%s' % feature].transform(lambda x: x.shift(20).fillna(0))\n            #merged_data['returnsOpenPrevMktres10_lag_2'] = merged_data.groupby('assetCode')['returnsOpenPrevMktres10'].transform(lambda x: x.shift(20).fillna(0))\n            #merged_data['returnsOpenPrevMktres10_lag_3'] = merged_data.groupby('assetCode')['returnsOpenPrevMktres10'].transform(lambda x: x.shift(30).fillna(0))\n            #merged_data['returnsOpenPrevMktres10_lag_4'] = merged_data.groupby('assetCode')['returnsOpenPrevMktres10'].transform(lambda x: x.shift(40).fillna(0))\n    #merged_data['returnsOpenPrevRaw10_lag_1'] = merged_data.groupby('assetCode')['returnsOpenPrevRaw10'].transform(lambda x: x.shift(10).fillna(x.mean()))\n    #merged_data['returnsOpenPrevMktres10_lag_2'] = merged_data.groupby('assetCode')['returnsOpenPrevMktres10'].transform(lambda x: x.shift(20).fillna(x.mean()))\n    #merged_data['returnsOpenPrevRaw10_lag_2'] = merged_data.groupby('assetCode')['returnsOpenPrevRaw10'].transform(lambda x: x.shift(20).fillna(x.mean()))\n    #merged_data['returnsOpenPrevMktres10_lag_3'] = merged_data.groupby('assetCode')['returnsOpenPrevMktres10'].transform(lambda x: x.shift(30).fillna(x.mean()))\n    #merged_data['returnsOpenPrevRaw10_lag_3'] = merged_data.groupby('assetCode')['returnsOpenPrevRaw10'].transform(lambda x: x.shift(30).fillna(x.mean()))\n    #merged_data['returnsOpenPrevMktres10_lag_4'] = merged_data.groupby('assetCode')['returnsOpenPrevMktres10'].transform(lambda x: x.shift(40).fillna(x.mean()))\n    #merged_data['returnsOpenPrevRaw10_lag_4'] = merged_data.groupby('assetCode')['returnsOpenPrevRaw10'].transform(lambda x: x.shift(40).fillna(x.mean()))\n    #merged_data['returnsClosePrevMktres10_lag_1'] = merged_data.groupby('assetCode')['returnsClosePrevMktres10'].transform(lambda x: x.shift(10).fillna(x.mean()))\n    #merged_data['returnsClosePrevRaw10_lag_1'] = merged_data.groupby('assetCode')['returnsClosePrevRaw10'].transform(lambda x: x.shift(10).fillna(x.mean()))\n    #merged_data['returnsClosePrevMktres10_lag_2'] = merged_data.groupby('assetCode')['returnsClosePrevMktres10'].transform(lambda x: x.shift(20).fillna(x.mean()))\n    #merged_data['returnsClosePrevRaw10_lag_2'] = merged_data.groupby('assetCode')['returnsClosePrevRaw10'].transform(lambda x: x.shift(20).fillna(x.mean()))\n    #merged_data['returnsClosePrevMktres10_lag_3'] = merged_data.groupby('assetCode')['returnsClosePrevMktres10'].transform(lambda x: x.shift(30).fillna(x.mean()))\n    #merged_data['returnsClosePrevRaw10_lag_3'] = merged_data.groupby('assetCode')['returnsClosePrevRaw10'].transform(lambda x: x.shift(30).fillna(x.mean()))\n    #merged_data['returnsClosePrevMktres10_lag_4'] = merged_data.groupby('assetCode')['returnsClosePrevMktres10'].transform(lambda x: x.shift(40).fillna(x.mean()))\n    #merged_data['returnsClosePrevRaw10_lag_4'] = merged_data.groupby('assetCode')['returnsClosePrevRaw10'].transform(lambda x: x.shift(40).fillna(x.mean()))\n    \n    return merged_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b13c630de23737bb884c25d85a4fe19072ba219","trusted":true},"cell_type":"code","source":"# Removing outliers (defined as data m times std. away from its mean)\ndef remove_outliers(data, columns, m=3):\n    \"\"\"\n    type data: DataFrame\n    type columns: list[str]\n    type m: int\n    rtype: DataFrame\n    \"\"\"\n    normal = np.asarray([True] * len(data))\n    for i in range(len(columns)):\n        normal = normal & (abs(data[columns[i]] - np.mean(data[columns[i]])) <= m * np.std(data[columns[i]]))\n    return data[normal]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"707a4f71d669604f902e12f9af2b08bb10b39065","trusted":true},"cell_type":"code","source":"# Create lag features for prediction data\ndef rolling_lag_features(cdf, rolling, lag_features_window_10, lag_features_window_1=[], n_lag=2):\n    \"\"\"\n    type cdf: DataFrame (merged data on a prediction day)\n    type rolling: DataFrame (saved historical market data)\n    \"\"\"\n    ## save merged_data from current and previous 10 days & keep rolling, for the purpose of creating lag Prev10 returns\n    # step 1: concatenate data from current day to the DataFrame 'rolling'\n    rolling = pd.concat([rolling, cdf[['time', 'assetCode'] + lag_features_window_10 + lag_features_window_1]]).reset_index(drop=True)\n    # step 2: fill in one-lag feature in cdf by merging from 'rolling'\n    #print(day, len(rolling['time'].unique())) # debug\n    rollingtime = rolling['time'].unique()\n    \n    for i in range(n_lag):\n        timelag10 = rollingtime[10 * (n_lag - i - 1)]\n        cdf = pd.merge(left = cdf, right = rolling.loc[rolling['time'] == timelag10, ['assetCode'] + lag_features_window_10], how='left', on='assetCode', suffixes=('', '_rolling_%s'%(i+1)))\n        #timelag1, timelag2 = rollingtime[10], rollingtime[0]\n        #cdf = pd.merge(left = cdf, right = rolling.loc[rolling['time'] == timelag1, ['assetCode'] + lag_features], how='left', on='assetCode', suffixes=('', '_rolling_1'))\n        #cdf = pd.merge(left = cdf, right = rolling.loc[rolling['time'] == timelag2, ['assetCode'] + lag_features], how='left', on='assetCode', suffixes=('', '_rolling_2'))\n        for feature in lag_features_window_10:\n            cdf['%s_lag_%s' % (feature, i+1)] = cdf['%s_rolling_%s' % (feature, i+1)]\n            #cdf['%s_lag_2' % feature] = cdf['%s_rolling_2' % feature]\n            cdf = cdf.drop(columns=['%s_rolling_%s' % (feature, i+1)])\n        if lag_features_window_1:\n            timelag1 = rollingtime[-i-1]\n            cdf = pd.merge(left = cdf, right = rolling.loc[rolling['time'] == timelag1, ['assetCode'] + lag_features_window_1], how='left', on='assetCode', suffixes=('', '_rolling_%s'%(i+1)))\n            for feature in lag_features_window_1:\n                cdf['%s_lag_%s' % (feature, i+1)] = cdf['%s_rolling_%s' % (feature, i+1)]\n                cdf = cdf.drop(columns=['%s_rolling_%s' % (feature, i+1)])\n            \n    # step 3: drop rows corresponding to the oldest day from 'rolling'\n    rolling = rolling.drop(rolling[rolling['time'] == rollingtime[0]].index)\n    return cdf, rolling","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae0061d0a64c104dea19f5c4e3e4db06f2b05525","trusted":true},"cell_type":"code","source":"# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88faa70f4034e4f75819b84933a312c553cc4dd2","trusted":true},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"965abf4986a3ba2de8fa5859113c75822e546cc8","trusted":true},"cell_type":"code","source":"# for debugging purpose\n#rownum = 1500000\n#market_train_df, news_train_df = market_train_df[0:rownum], news_train_df[0:rownum]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f604596ba98da1fe11ff43af93ef43487c1f2360","trusted":true},"cell_type":"code","source":"# Merging data\n#merge_start = time.time()\nmerged_data = merge_data(market_train_df, news_train_df)\n#print(\"Merging data took %.0f minutes.\" % math.ceil((time.time() - merge_start)/60))\n# train post-crisis samples\nmerged_data = merged_data[merged_data['time'] >= datetime(2009, 7, 1).date()]\ndel market_train_df, news_train_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c259ab3b0cbc521d68607ce8978cc0aeb50f11e","trusted":true},"cell_type":"code","source":"# Handling outliers\nRaw_cols = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nMktres_cols = ['returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\noutliercols = ['volume', 'close', 'open'] + Raw_cols + Mktres_cols\nmerged_data = remove_outliers(merged_data, outliercols, m=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da5f6a6d028d32a160a20ab23e8df4bdcd6661ad","trusted":true},"cell_type":"code","source":"# Preparing data for training\n#prepare_start = time.time()\nlag_features_window_10 = ['returnsOpenPrevMktres10', 'returnsOpenPrevRaw10', 'returnClosePrevMkt10', 'returnOpenPrevMkt10']\nlag_features_window_1 = ['returnClosePrevMkt1', 'returnOpenPrevMkt1'] # creating lag features for the top six important features\nn_lag = 2\ncdf = prepare_data(merged_data, 'train', lag_features_window_10, lag_features_window_1, n_lag)\n#print(\"Preparing data for training took %.0f minutes.\" % math.ceil((time.time() - prepare_start)/60))\ndel merged_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7defff3b313be8c0f8cd95d442f04431b685c627","trusted":true},"cell_type":"code","source":"# Building training set\ntargetcol = 'returnsOpenNextMktres10'\ncdf[targetcol] = (cdf[targetcol] > 0).astype(int) # we be classifying\ndropfeatures = ['noNews', 'noveltyCount5D', 'urgency', 'noveltyCount24H', 'noveltyCount12H'] # drop last five features\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode', 'assetName', 'universe'] + [targetcol] + dropfeatures]\n\n## Train 1: with valid set\ndates = cdf['time'].unique()\ntrain = range(len(dates))[:int(0.95*len(dates))]\nval = range(len(dates))[int(0.95*len(dates)):]\n\n# train data\nXt = cdf[traincols].loc[cdf['time'].isin(dates[train])].values\nYt = cdf[targetcol].loc[cdf['time'].isin(dates[train])].values\n\n# validation data\nXv_with_time = cdf[traincols + ['time']].fillna(0).loc[cdf['time'].isin(dates[val])]\nYv = cdf[targetcol].fillna(0).loc[cdf['time'].isin(dates[val])].values\n\n#print(Xt.shape, Xv.shape)\n##---\n\n## Train 2: no valid set\n#allTrainData = cdf[traincols].fillna(0).values\n#allTrainLabels = cdf[targetcol].fillna(0).values\n##---\n\nrolling = cdf.loc[cdf['time'] >= cdf.time.unique()[-10 * n_lag], ['time', 'assetCode'] + lag_features_window_10 + lag_features_window_1]  # to be used in prediction part\ndel cdf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8975d4329ecaac360cb89ffcd27ee1a2e3b399af","trusted":true},"cell_type":"code","source":"# Feature scaling\nsc = StandardScaler()\n\n## Train 1: with valid set\nXt = sc.fit_transform(Xt)\n#Xv = sc.transform(Xv)\n\n## Train 2: no valid set\n#allTrainData = sc.fit_transform(allTrainData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81e0d4127374ace1f7f8bc254af8fbf09eb7a6c8"},"cell_type":"code","source":"def find_combinations(grid_params):\n    params = []\n    for key in grid_params.keys():\n        params.append(grid_params[key])\n    return list(product(*params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47d27300b45e445d224945560eb82593b984ceb8"},"cell_type":"code","source":"def post_scaling(pred_Y):\n    \"\"\"\n    type pred_Y: array\n    rtype: array\n    \"\"\"\n    mean, std = np.mean(pred_Y), np.std(pred_Y)\n    pred_Y = (pred_Y - mean) / std / 8\n    return np.clip(pred_Y, -1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f89d46bc6d4b19627289b1310cab82464efd293"},"cell_type":"code","source":"def score(x, y, pred_y):\n    all_time = x['time'].values\n    unique_time = x['time'].unique()\n    daily_score = np.zeros(len(unique_time))\n    for i in range(len(unique_time)):\n        index = []\n        for j in range(x.shape[0]):\n            if all_time[j] == unique_time[i]:\n                index.append(j)\n        daily_score[i] = (y[index]*pred_y[index]).sum()\n    return np.mean(daily_score)/np.std(daily_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b738d764555f044bddd0fe008d905da9b708de8"},"cell_type":"code","source":"def grid_search(X_train, Y_train, X_val, Y_val, grid_params):\n    combinations = find_combinations(grid_params)\n    print(\"Total number of candidates = %d\" % len(combinations))   \n    # Default parameters\n    hyper_params = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n          \"num_leaves\" : 32,\n          \"max_depth\": -1,\n          \"max_bin\": 512,\n          'subsample_for_bin': 200,\n          'subsample': 0.7,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.65,\n          'reg_alpha': 1.2,\n          'reg_lambda': 1,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          \"learning_rate\" : 0.01}\n    # Iterate over all candidates\n    cv_results = {'hyper_params':[], 'score':[]}\n    params_keys = list(grid_params.keys())\n    for i in range(len(combinations)):\n        out_info = \"\"\n        for j in range(len(combinations[i])):\n            value = combinations[i][j]\n            key = params_keys[j]\n            out_info += key + \": \" + str(value) + \" \"\n            # Update params\n            hyper_params[key] = value\n        # Train and Calibrate\n        print(\"Candidate %d / %d ... %s\" % (i+1, len(combinations), out_info))\n        # Drop the last column for time\n        X_val_numbers = sc.transform(X_val[traincols].values)\n        lgbmodel = lgb.train(hyper_params, train_set=lgb.Dataset(X_train, Y_train), valid_sets=lgb.Dataset(X_val_numbers, Y_val), num_boost_round=1000, early_stopping_rounds=50, verbose_eval=500)\n        pred = post_scaling(lgbmodel.predict(X_val_numbers))\n        ir = IsotonicRegression(out_of_bounds = 'clip')\n        ir.fit(pred, Y_val)\n        pred_calibrated = post_scaling(ir.transform(pred))\n        # Score before calibration on the validation data\n        before_calibration_score = score(X_val, Y_val, pred)\n        print(before_calibration_score)\n        after_calibration_score = score(X_val, Y_val, pred_calibrated)\n        print(after_calibration_score)    \n        cv_results['hyper_params'].append(hyper_params)\n        cv_results['score'].append(after_calibration_score)\n    return cv_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11ebaeca100b90754d282356c3bb4f7e2db35285"},"cell_type":"code","source":"print('Grid Search with Cross-Validation')\ngridParams = {\n    'learning_rate': [0.005],\n    'num_leaves': [32,64,128],\n    'colsample_bytree' : [0.6,0.65,0.7],\n    'subsample' : [0.7,0.75,0.8],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2]}\ncv_results = grid_search(Xt,Yt,Xv_with_time,Yv,gridParams)\nbest_score = max(cv_results['score'])\nbest_index = cv_results['score'].index(best_score)\nbest_params = cv_results['hyper_params'][best_index]\nprint(\"Best Accuracy: %f\" % best_accuracy)\nprint(\"best learning_rate = %f\" % best_params['learning_rate'])\nprint(\"best num_leaves = %f\" % best_params['num_leaves'])\nprint(\"best colsample_bytree = %f\" % best_params['colsample_bytree'])\nprint(\"best subsample = %f\" % best_params['subsample'])\nprint(\"best reg_alpha = %f\" % best_params['reg_alpha'])\nprint(\"best reg_lambda = %f\" % best_params['reg_lambda'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"505d4a73a07ac97170bce8016b8cdef0ec1bb6cb"},"cell_type":"code","source":"# Retrain model using the best_params\nXv = sc.transform(Xv_with_time[traincols].values)\nlgbmodel = lgb.train(best_params, train_set=lgb.Dataset(Xt, Yt), valid_sets=lgb.Dataset(Xv, Yv), num_boost_round=2000, early_stopping_rounds=200, verbose_eval=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6c4e1a976ef4f291711e53f029e0deaffd16044"},"cell_type":"code","source":"# Calibrate prediction\npred = lgbmodel.predict(Xv)\nir = IsotonicRegression(out_of_bounds = 'clip')\nir.fit(pred,Yv)\npred_calibrated = ir.transform(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38f284ed8cad37d90e469eb8aa6faa0905690f76"},"cell_type":"code","source":"def cdf(data, calibrated_data):\n    # sort the data:\n    data_sorted = np.sort(data)\n    calibrated_data_sorted = np.sort(calibrated_data)\n    # calculate the proportional values of samples\n    p_data = 1. * np.arange(len(data)) / (len(data) - 1)\n    p_calibrated_data = 1. * np.arange(len(calibrated_data)) / (len(calibrated_data) - 1)\n    # plot the sorted data:\n    plt.plot(data_sorted, p_data, 'b')\n    plt.plot(calibrated_data_sorted, p_calibrated_data, 'r')\n    plt.xlabel('$x$')\n    plt.ylabel('$p$')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba88645bb124e75416e7161f178eaa6b037a0e82"},"cell_type":"code","source":"cdf(pred, p_calibrated)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9048fc2161a0d0da6b1f0bbdfce4c642c07ecd5e","scrolled":true,"trusted":true},"cell_type":"code","source":"\"\"\"\n#######################################################\n##\n## LightGBM\n##\n#######################################################\n#import lightgbm as lgb\n\n# sklearn tools for model training and assesment\n#from sklearn.model_selection import train_test_split\n#from sklearn.metrics import (roc_curve, auc, accuracy_score)\n#from sklearn.model_selection import GridSearchCV\n\nprint ('Training lightgbm')\n# Create parameters to search\ngridParams = {\n    'learning_rate': [0.005,0.01,0.05],\n    'num_leaves': [16,32,64,128],\n    'colsample_bytree' : list(np.linspace(0.6, 0.7, 3)),\n    'subsample' : list(np.linspace(0.7, 0.9, 5)),\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4]}\n\n# Grid Search\n\n# GridSearch and Calibration of classifier\nparams = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n          \"num_leaves\" : 32,\n          \"max_depth\": -1,\n          \"max_bin\": 512,\n          'subsample_for_bin': 200,\n          'subsample': 0.7,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.65,\n          'reg_alpha': 1.2,\n          'reg_lambda': 1,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          \"learning_rate\" : 0.01}\n\n\nmodel = lgb.LGBMClassifier(boosting_type= 'gbdt',\n                           objective = 'binary',\n                           metric = 'binary_logloss',\n                           silent = True,\n                           max_depth = params['max_depth'],\n                           max_bin = params['max_bin'],\n                           subsample_for_bin = params['subsample_for_bin'],\n                           subsample = params['subsample'],\n                           subsample_freq = params['subsample_freq'],\n                           min_split_gain = params['min_split_gain'],\n                           min_child_weight = params['min_child_weight'],\n                           min_child_samples = params['min_child_samples'],\n                           learning_rate=params['learning_rate'])\n\n# Create the grid\n#grid = GridSearchCV(model, gridParams,\n#                    verbose=10,\n#                    cv=5,\n#                    n_jobs=1)\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)\n\nlgtrain = lgb.Dataset(allTrainData, allTrainLabels)\n#lgbmodel = None\nlgbmodel = lgb.train(params, train_set=lgtrain, num_boost_round=10000, verbose_eval=False)\npred = lgbmodel.predict(allTrainData)\nir = IsotonicRegression(out_of_bounds = 'clip')\nir.fit(pred,allTrainLabels)\npred_calibrated = ir.transform(pred)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cdb7aa1eb1919aa3137aaf964a6374b13736e12","trusted":true},"cell_type":"code","source":"##### scaling\ndef post_scaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)/ (std * 8)\n    return np.clip(df,-1,1)\n############################################################\nprint(\"generating predictions...\")\npreddays = env.get_prediction_days()\n#day = 0 # debug\n#pred_start = time.time()\nlogging.getLogger().disabled = True\nsaved_data = pd.DataFrame([])\nfor marketdf, newsdf, predtemplatedf in preddays:\n    #day += 1 # debug\n    merged_data = merge_data(marketdf, newsdf, split_by_year=False) # merge data\n    cdf = prepare_data(merged_data, 'pred', lag_features_window_10, lag_features_window_1, n_lag) # prepare merged data for prediction\n    cdf, rolling = rolling_lag_features(cdf, rolling, lag_features_window_10, lag_features_window_1, n_lag)\n    Xp = sc.transform(cdf[traincols].fillna(0).values) # extract features columns and scale\n    preds = ir.transform(lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration)) # calibrate\n    preds = preds*2 - 1\n    predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\n    predtemplatedf.loc[predtemplatedf['assetCode'].isin(predsdf.ast), 'confidenceValue'] = predsdf['conf'].values\n    env.predict(predtemplatedf)\n    \"\"\"\n    # Incremetal learning the model\n    saved_data = pd.concat([saved_data, cdf], ignore_index=True)\n    saved_data_time = saved_data['time'].unique()\n    if (saved_data_time.shape[0] == 11):\n        # Data for day 1\n        day1 = saved_data.loc[saved_data.time == saved_data_time[0]]\n        # Data for day 10\n        day10 = cdf[['assetCode', 'returnsOpenPrevMktres10']]\n        day10.rename(columns={'returnsOpenPrevMktres10': 'label'}, inplace=True)\n        # Merge data from day 1 and day 10\n        merged = pd.merge(left=day1, right=day10, on=['assetCode'], how='inner')\n        incremental_train_data = sc.transform(merged[traincols].fillna(0).values)\n        incremental_train_labels = (merged['label'] > 0).astype(int).fillna(0).values\n        lgbmodel = lgb.train(params, init_model=lgbmodel, train_set=lgb.Dataset(incremental_train_data, incremental_train_labels), num_boost_round=10, verbose_eval=False, keep_training_booster=True)\n        saved_data.drop(saved_data[saved_data.time == saved_data_time[0]].index, inplace=True)\n    \"\"\"\n#print(\"Prediction took %.0f minutes.\" % math.ceil((time.time() - pred_start)/60))\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1723351e32d4f94033087d8d90a7c3bec010e8a1","trusted":true},"cell_type":"code","source":"# Feature importance\nfeature_imp = pd.DataFrame(sorted(zip(lgbmodel.feature_importance(importance_type='gain'), traincols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 15))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2f53bf4af246386341754e9ce27d86161524c92","trusted":true},"cell_type":"code","source":"# records\n# based on V10: turn off time tracker, replace Fang's training method and parameters with mine, same features: 0.65015\n# found a bug, drop the lag features: 0.67390\n# use Fang's training methods (the only difference from V10 is dropping lag features; the diff from V7 is adding five news feature \n# ... adding daytrend & PrevMkt, and removing outliers, handling missing values, training post-crisis data, pre-scaling): 0.70359\n# fix the bug (based on one-lag): 0.68992\n# add two lags, fillna with 0 in creating lags, fix the reset_index() bug in last version: \n# based on 0.75235 version, drop post-scaling: ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd438dad2b57daf991f064e64d4efa4bdbc399e5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}